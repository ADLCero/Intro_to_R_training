---
title: "Introduction to R Programming"
author: "Amyel Dale Cero"
date: "2025-09-21"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

Notes: 

1. If you are using this document in `.Rmd` format, you can press the "play" button on the upper right corner of the chunk of code (i.e., gray section) to run the code and see the results.

2. If you are using this document in its `.html` format, you can copy-paste the code in an R script and see how it works.


# 1. Assigning and using variables

We can use the `=` sign to assign values to variables but the `<-` operator is conventionally preferred.

```{r}

# "Initializing" a variable
# Variable <- Value

A <- 1
B <- 2
C <- 3

print(A)
print(B)
print(C)
```

We can perform basic math operations by using the variables instead of the numbers assigned to them:

```{r}

D <- A + B
D

```

We can also overwrite variables:

```{r}

# We can also overwrite variables

C <- 2 * A * B ^ B
C

```

Note how the value of C changes from 3 to 8, subsequently changing the results of succeeding operations:

```{r}

C + D

```

**Using variables:**

- Links output from one line of code to a later line
- Minimizes errors
- Makes the code more generalizable
- Allows automation of data processing
- Makes the code easier to update and revise


# 2. Data types in R

**Variables do not have to be numbers.**

There are multiple data types in R and in other computer languages. The standard data types we will often encounter are:

## a. Numeric

```{r}

# Subcategories: integer & double

E <- 525600

```

## b. Logic

```{r}

# TRUE or FALSE

Outcome <- TRUE

```

## c. Character

```{r}

# Also known as string; written with quotation marks around them

BestProfession <- "Engineering"

```

We can print the variables to see the values assigned to them.

```{r}

print(E)
print(Outcome)
print(BestProfession)

```

To know the type of the variable, we can use `class()`

```{r}

class(E)
class(Outcome)
class(BestProfession)

```

## d. Factor

A factor in R is a data structure used to handle categorical variables. It is especially useful when the variable represents a fixed number of categories (example: male vs female; low-medium-high).

Factors are internally stores as integers with labels. They can also be "ordered" or "unordered".

```{r}

# Creating a factor
gender <- factor(c("Male", "Female", "Female", "Male", "Non-binary"))

# Check levels
levels(gender)

```

We can also specify the levels and orders of factors.

```{r}

# Custom levels and order
level_ordered <- factor(c("Low", "High", "Medium"),
                        levels = c("Low", "Medium",
                                   "High"), 
                        ordered = TRUE)
level_ordered

```


```{r}

# Check if it's ordered
is.ordered(level_ordered)

```

## Converting between different types of variables

```{r}

# From character to factor
x <- as.factor(c("A", "B", "A"))
x
class(x)

```

```{r}

# From factor to character
x <- as.character(x)
x
class(x)

```

```{r}

# From character to numeric 
y <- c("1", "2", "3")
y <- as.numeric(y)
y
class(y)

```

## EXERCISE 1: Solving problems using variables in R.

**Use variables in a script to solve for the number of liters of water needed annually by a town.**

- Each person uses on average 120 liters of water per day.
- There are 10,000 residents in the town.
- A golf course uses on average 1,400,000 liters of water per month.
- Presume an average month is 30 days.
There are three (3) golf courses in the town.

**How much water does the town use per year?**

```{r}

# There are many ways to approach this problem. Here's one example:

# Step 1: Define the variables

Population <- 10000 # people in the town
Population_LPD <- 120 # water consumption per person (liters per day)

Golf_course_LPM <- 1400000 # water consumption of the golf course per month (liters)
Golf_course_no <- 3 # number of golf courses in the town

Days_year <- 365 # number of days in a year
Days_month <- 30 # number of days in a month

# Step 2: Do the computation

# Compute for the water consumption by:

# a. All people
People_use <- Population * Population_LPD * Days_year

# b. Golf course
Golf_use <- Golf_course_no * ((Golf_course_LPM/Days_month) * Days_year)

# c. Total use
Total_use <- People_use + Golf_use

print(Total_use)

```

Can we add text (characters) to the printed output so that it provides more information?

Using the `paste()` function allows us to do that.

```{r}

print(paste("The total water consumption in the town is", Total_use, "liters per year."))

```

# 3. Data structures in R

We were able to assign single values to variables. But what if we have a number of related values?

```{r}

Crop1 <- "rice"
Crop2 <- "corn"
Crop3 <- "sugarcane"
Crop4 <- "cassava"

```

It can be a bit tedious to assign each value to a different variable. Instead, what we can do is to group them:

```{r}

Crops <- c("rice", "corn", "sugarcane", "cassava")
Crops

```

```{r}

# We can also use the assigned variables to group them together:

Crops2 <- c(Crop1, Crop2, Crop3, Crop4)
Crops2

```

## a. Vectors

A vector is a data structure that holds elements of the same data type.

Note the syntax for a vector: c(Item1, Item2, ...)
c = concatenate = link things together in a chain or series

```{r}

# Numeric vector
v1 <- c(1, 2, 3, 4, 5)

# Character vector
v2 <- c("apple", "banana", "grapes", "cherry", "strawberry")

# Logical vector
v3 <- c(TRUE, FALSE, TRUE, FALSE, FALSE)

```

Other ways to create vectors:

```{r}

# Sequence of numbers
v_seq <- seq(0, 50, 2) # Sequence of numbers from 0 to 50, by 2s

# Repeating values
v_rep <- rep(5, times = 4)

```

We can check the type and length of vectors:

```{r}

length(Crops)     # number of elements
typeof(Crops)     # data type
is.vector(Crops)  # TRUE if it is a vector

```

### a.1. Accessing values in a vector

We can use `VectorName[index#]` to isolate the desired item. ("Indexing")

```{r}

# Using the Crops vector:

Crops[1]    # Gets the first element
Crops[4]    # Gets the fourth element

```

```{r}

# Accessing multiple values in a vector

Crops[1:2]

```

```{r}

# Overwriting an element in a vector using indexing

Crops[3] <- "dragonfruit"
Crops

```

### a.2. Vector operations

R is vectorized: it is designed to perform operations on entire vectors of data at once instead of doing one element at a time.

```{r}

x <- c(1, 2, 3)
y <- c(4, 5, 6)

x + y   # 5 7 9
x * 2   # 2 4 6
x > 2   # FALSE FALSE TRUE

```


**Vectors** contain groups of objects in one dimension (column or row).

**Matrices** contain groups of objects in two dimensions (a grid). 

**Arrays** contain groups of objects in any number of dimensions (i.e., vectors and matrices are just specific types of an array).  


## b. Matrix

A matrix is a 2D structure where all elements must be of the same data type.

There are many ways to initialize a matrix.

```{r}

# Creating a matrix (2D array)
# Option 1: using array() (since a matrix is an array)

m1 <- array(data = 1:10, dim = c(5, 2))
m1

```

```{r}

# Option 2: using matrix()

m2 <- matrix(data = 1:10, nrow = 5, byrow = FALSE)
m2

```
We can use `class()` to see the object's class (its behavior or type as seen by users) and/or `typeof()` to see the internal storage type that R used for the object.

```{r}

class(m1)
typeof(m1)

class(m2)
typeof(m2)

```
We can also create a matrix by binding vectors.

```{r}

# Column-bind
m3 <- cbind(c(1,2), c(3,4))

# Row-bind
m4 <- rbind(c(1,2), c(3,4))

print(m3)
print(m4)

# Try to spot the difference between doing a cbind versus rbind:

```
### b.1. Accessing elements in a matrix

```{r}

# Create a sample matrix

m5 <- matrix(1:20, nrow = 4)
m5

```

```{r}

# Access elements in the matrix

m5[1, 2]   # Row 1, Column 2
m5[ , 2]   # Entire column 2
m5[4, ]    # Entire row 4

```

### b.2. Matrix operations

```{r}

# Create matrices

m6 <- matrix(1:4, nrow = 2)
m7 <- matrix(5:8, nrow = 2)

m6
m7

```

```{r}

m6 + m7   # Element-wise addition
m6 * m7   # Element-wise multiplication

```
### b.3. Useful matrix functions

```{r}

dim(m5)      # dimensions (number of rows, number of columns)
nrow(m5)     # number of rows
ncol(m5)     # number of columns
rowSums(m5)  # sum of each row
colSums(m5)  # sum of each column
rowMeans(m5) # average of each row
colMeans(m5) # average of each column
t(m5)        # transpose

```

## c. Data Frame

Vectors and matrices require that their elements are of the same data type.

What if we want to combine different data types?

A **data frame** is a 2-dimensional table-like structure
- Each column is a vector (of the same length)
- Different columns can have different data types (numeric, character, factor, etc.),

It is the most commonly used structure for data sets in R (like Excel sheets).

```{r}

df <- data.frame(
  crop = c("rice", "corn", "sugarcane", "dragonfruit", "cassava"),
  weight_kg = c(100, 250, 80, 550, 150),
  days_in_storage = c(10, 15, 8, 9, 5)
)

df

```
```{r}

# To view the df in a separate tab:
View(df)

```

### c.1. Accessing data frame elements

```{r}

df$crop     # use dollar sign then the name of the column

```

```{r}

df[1, 2]            # by row and column index

```

```{r}

df[ , "weight_kg"]  # all rows of column weight_kg

```

```{r}

df[1, ]             # entire first row

```

```{r}

# Using subset()

subset(df, weight_kg > 100)

```

### c.2. Inspecting a data frame

```{r}

# dimensions (number of rows, number of columns)
dim(df)          

```

```{r}

# structure
str(df)   

```

```{r}

# summary statistics
summary(df)

```

```{r}

# column names
names(df)

```

### c.3. Modifying data frames

```{r}

# Adding a new column
df$storage_room <- c(1, 1, 2, 3, 4)
df

```
```{r}

# Renaming a column
# a. Rename a single column by name
names(df)[names(df) == "weight_kg"] <- "weight_tons"
df

```
```{r}

# b. Rename by column position
names(df)[2] <- "weight_kg"
df

```
```{r}

# c. Rename multiple columns
names(df) <- c("crop_name", "weight", "number_of_days_stored", "room_no")
df

```
## d. Lists

A list in R is a flexible data structure that can hold elements of different types and lengths, including:

- vectors
- matrices
- data frames
- and even other lists

They are building blocks of more complex R objects (like models).

```{r}

# Creating a simple list
list1 <- list(1, "hello", TRUE, c(2, 3, 4))
list1

```
```{r}

# Creating a named list
list2 <- list(crop = "rice",
              weight_kg = c(100, 250, 80),
              status_in_storage = c(TRUE, FALSE, TRUE),
              room_no = "1a")
list2

```
### d.1. Accessing list elements

```{r}

# 1. Using $ for named elements

list2$weight_kg

```


```{r}

# 2. Using double brackets [[]]

list2[[2]]                # gets the second element
list2[["weight_kg"]]      # gets the "weight_kg" (which is also the second element)

```


# 4. More about data frames in R

We will focus in working on data frames since it will be the usual data structure of most data sets that we will be using in our work.

R comes with several built-in data frames that are perfect for learning, testing, and practicing data analysis.

These are preloaded with base R or available in standard packages like `datasets`.

```{r}

# How to see all built-in data sets in R
data()

# A separate tab showing all built-in data sets will come out. View all available data.

```

```{r}

# Let us use ChickWeight = Weight versus age of chicks on different diets

# STEP 1: Load the data set

data("ChickWeight")  # Load the data set

```

```{r}

# STEP 2: See the documentation of the data set

?ChickWeight

# A documentation will appear in the Help tab.

```

```{r}

# STEP 3: Explore the data

head(ChickWeight)     # shows the first few rows

```

```{r}

tail(ChickWeight)    # shows the last few rows

```

```{r}

str(ChickWeight)    # shows the structure

```

```{r}

summary(ChickWeight)   # shows the summary statistics

```
Note that `summary()` can give us the summary statistics of the data frame but we can also use separate functions to do this, if needed:

```{r}

# mean
mean_weight <- mean(ChickWeight$weight)

# median
median_weight <- median(ChickWeight$weight)

# mode
mode_weight <- mode(ChickWeight$weight)

# standard deviation
sd_weight <- sd(ChickWeight$weight)

print(mean_weight)
print(median_weight)
print(mode_weight)
print(sd_weight)

# We assign the values to variables so they are saved in our environment and we can use them later in other computations.

```

**We can also used the base R `plot()` function to see the matrix of scatterplots, also called pairs plot.**

```{r}

# STEP 4: Explore the relationships between the numeric columns in the data frame

plot(ChickWeight)

```

This is a scatterplot matrix of all numeric columns in the data frame. Each cell shows a scatterplot between two numeric variables.

Note that the plot in row 1, column 2 is just a mirror of row 2, column 1 (i.e., axes flipped).

**What to look for in each plot:**

**1. Linear patterns**

- If X increases and Y also increases = positive correlation
- If X increases and Y decreases → negative correlation

**2. Curved patterns**

- Indicates a non-linear relationship.

**3. Clusters**

- Points forming distinct groups might indicate the presence of categorical groups or outliers.

**4. Outliers**

- Points far from the main cloud = potential data issues or interesting cases


# 5. Importing data sets to R using base R functions

## a. CSV files

Why is CSV file preferred over Excel?

1. Comma-separated values (CSV) is just raw data in plain text
2. Works across any programming language (R, Python, SQL, JavaScript, etc.)
3. No hidden formatting, just data = reduces unexpected behavior when importing
4. Not proprietary! We don’t need Excel or any licensed software to open or edit a CSV.

We mainly use the `read.csv()` to bring in .csv files into R.

The tricky part here is identifying the file path to the data.

```{r}

# We can use getwd() to help identify our working directory, where the .csv file (preferably) should also be located.

getwd()

```

```{r}

# Let us try importing the provided CSV file.
# Provide the file path to the data:

data <- read.csv("/Users/amyeldalecero/R/TRAINING/Intro_to_R_training/00_Data/data.csv", 
                     header = TRUE)

# Note that this code will not work for you because your file will have a different file path.

# You will have to revise the line of code above to make it (and the rest of the code below) to work.

```

## b. TXT files

We can use `read.table()` to import text files into R.

## c. Excel files

Base R cannot read Excel files directly. We will need external packages like `readxl` or `openxlsx`.

What we can do is to save the Excel sheet as a .csv -- can be a problem when data is saved in multiple tabs!

**Important reminders:**

1. Use absolute paths
`C:/Users/yourname/Documents/file.csv`

2. Or relative paths (relative to your **working directory**): `data/file.csv`


# 6. Exploring the imported data

Once we have imported the file, the first step is to always explore it.

```{r}

# Explore 'data'

dim(data)

```

```{r}

str(data)

```

```{r}

head(data)

```

```{r}

tail(data)

```
```{r}

summary(data)

```

```{r}

plot(data)

```

```{r}

# See the column names
names(data)

```
```{r}

# Remove duplicate rows
clean_data <- data[!duplicated(data), ]

```

```{r}

# Note the changes in the dimensions
dim(data)
dim(clean_data)

```

```{r}

# Remove any row with one or more NAs
clean_data_NA <- na.omit(clean_data)
dim(clean_data_NA)

```

# 7. Data visualization with base R

Useful references:
https://r-graph-gallery.com/base-R.html
https://www.sthda.com/english/wiki/r-base-graphs


## a. Basic plot

```{r}

# Horsepower versus price

plot(x = clean_data_NA$Engine.HP,
     y = clean_data_NA$MSRP)

```

```{r}

# Adding more elements to make the plot look better

plot(x = clean_data_NA$Engine.HP,
     y = clean_data_NA$MSRP,
     main = "Horsepower vs Minimum Selling Retail Price", # title
     xlab = "engine horsepower", # x-axis title
     ylab = "minimum selling retail price") # y-axis title

```

## b. Boxplot

Used to visualize the distribution of a numeric variable showing its median, quartiles, range, and potential outliers.

```{r}

boxplot(clean_data_NA$MSRP, 
        ylab = "price")

```

## c. Histogram

Used to visualize the distribution of a numeric variable by dividing it into bins (intervals) and counting how many values fall into each bin

```{r}

hist(clean_data_NA$MSRP,
     main = "Histogram of price",  # title
     xlab = "Value",               # x-axis label
     ylab = "Frequency",           # y-axis label
     col = "lightblue",            # color
     border = "black",             # border color
     breaks = 5)                   # number of breaks

```


# 8. Installing packages in R

Packages provide additional functions, datasets, and tools that are not included in base R

**Installing Tidyverse**

`tidyverse` is a collection of R packages designed for data science. It includes tools for data manipulation, visualization, importing, and cleaning.

```{r, eval = FALSE}

install.packages("tidyverse")

```

```{r}

# Load the library after installing the package so that we can access its functions

library(tidyverse)

```

## a. dplyr

Let us use the `mtcars` data set in R.The `mtcars` dataset is a built-in dataset in R that contains information about 32 different car models from the 1970s.

```{r}

# Check the dimensions of the data set
dim(mtcars)

```

```{r}

# Check the column names
colnames(mtcars)

```

```{r}

# View the first few rows
head(mtcars)

```
```{r}

# View the structure
str(mtcars)

```
```{r}

# Get the summary statistics
summary(mtcars)

```


```{r}

# Check for missing values

sum(is.na(mtcars))          # Total NA values
colSums(is.na(mtcars))      # NA per column

```
```{r}

# Use glimpse() from dplyr for a quick overview
glimpse(mtcars)

```

```{r}

# EXAMPLE 1: Filter cars with mpg greater than 20

mtcars %>% 
  filter(mpg > 20) %>% 
  head()

# %>% = pipe operator = mechanism for chaining operations, allowing the output of one function to be seamlessly passed as the input to the next

```

```{r}

# EXAMPLE 2: Summarize average mpg by number of cylinders

mtcars %>%
  group_by(cyl) %>%
  summarise(avg_mpg = mean(mpg),
            avg_hp = mean(hp),
            count = n())

```

## b. ggplot2

```{r}

# Basic syntax of ggplot()

ggplot(data = mtcars,          # data set
       aes(x = wt, y = mpg)) + # x and y
  geom_point()                 # geometry


```

```{r}

# Customization

ggplot(data = mtcars,               # data set
       aes(x = wt, y = mpg,         # x and y
           color = factor(cyl))) +  # color based on variable
  geom_point(size = 3)  +           # geometry
  geom_smooth(method = "lm") +      # linear regression line
  labs(                             # labels
    title = "MPG vs Weight",
    x = "Weight (1000 lbs)",
    y = "Miles per gallon",
    color = "Cylinders") +
  
  theme_minimal()                   # theme

```

```{r}

# Histogram of miles per gallon (mpg)

ggplot(mtcars, aes(x = mpg)) +
  geom_histogram(bins = 10, 
                 fill = "steelblue", 
                 color = "white") +
  theme_minimal()

```

```{r}

# Boxplot of mpg by cylinder

ggplot(mtcars, 
       aes(x = factor(cyl), y = mpg)) +
  geom_boxplot(fill = "orange") +
  labs(x = "Cylinders", 
       y = "Miles Per Gallon", 
       title = "MPG by Cylinder Count") +
  theme_minimal()

```

```{r}

# Barplot of car counts by number of gears

ggplot(mtcars, 
       aes(x = factor(gear))) +
  geom_bar(fill = "steelblue") +
  labs(title = "Count of Cars by Gear", 
       x = "Number of Gears", 
       y = "Count") +
  theme_minimal()

```

```{r}

# Facet plot: scatterplot faceted by transmission type

ggplot(mtcars, 
       aes(x = wt, y = mpg)) +
  geom_point() +
  facet_wrap(~ am, labeller = labeller(am = c("0" = "Automatic", "1" = "Manual"))) +
  labs(title = "MPG vs Weight by Transmission Type") +
  theme_minimal()

```

# ADDITIONAL EXAMPLES

## 1. Heat maps

1. Creating a heat map in base R

```{r}

# Convert the mtcars data set to a matrix
mtcars_matrix <- as.matrix(mtcars)
mtcars_matrix

```

```{r}

# Create a heatmap

heatmap(mtcars_matrix,
        main = "Heatmap of mtcars",
        col = heat.colors(256),
        scale = "column")

```

2. Creating a heatmap with ggplot2

```{r, eval = FALSE}

# Install additional package
install.packages("reshape2")

```

```{r}

# Load the library
library(reshape2)

```
```{r}

# Reshape the mtcars data for ggplot

# Add car names as a column
mtcars$car <- rownames(mtcars)
mtcars_melt <- melt(mtcars, id.vars = "car")

# Check the data frame
head(mtcars_melt)

```

```{r}

# Plot

ggplot(mtcars_melt, aes(x = variable, y = car, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "red") +
  theme_minimal() +
  labs(title = "Heatmap of mtcars dataset", x = "Variable", y = "Car Model") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


# R language equivalent of "Iris Flower" data analysis and machine learning

## 1. Install packages

```{r}

# Install packages

install.packages("RColorBrewer")  # Set a custom color palette like seaborn's 'husl'
install.packages("reshape2")     # For reshaping dataset


# MODELING LIBRARIES (similar to scikit-learn estimators)

install.packages("rsample")      # For separating training and testing sets
install.packages("caret")         # For ML: train/test split, training models, CV, accuracy, etc.

install.packages("nnet")          # For multinomial logistic regression
install.packages("e1071")         # For SVM and Naive Bayes
install.packages("kernlab")
install.packages("naivebayes")    # For Naive Bayes
install.packages("MASS")          # For LDA (Linear Discriminant Analysis)
install.packages("rpart")         # For Decision Trees
install.packages("class")         # For KNN
install.packages("klaR")          # Alternative Naive Bayes
install.packages("nnet")          # Neural networks / multinomial logistic regression (optional)


```

## 2. Load the libraries

```{r}

# Load libraries

library(dplyr)         
library(tidyr)         
library(readr)         
library(ggplot2)
library(RColorBrewer)
library(reshape2)
library(rsample)
library(caret)
library(nnet)
library(MASS)
library(class)
library(rpart)
library(naivebayes)
library(e1071)
library(kernlab)

```

## 3. Get the data

```{r}

# Get the data from GitHub
# Note that Iris data set is also built-in in R

# Define the URL
url <- "https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv"

# Define column names
col_names <- c("sepal-length", "sepal-width", "petal-length", "petal-width", "class")

# Read the CSV from the URL with custom column names
dataset <- readr::read_csv(url, col_names = col_names)

# View the first few rows
head(dataset)

```

```{r}

# Check the structure
# Let us use the version of the data that was downloaded from the web

str(dataset)

```

```{r}

# Check the summary of the dataset
summary(dataset)

```
```{r}

# Count the number of occurrences

dataset %>%
  count(class)

```

## 4. Clean the data

```{r}

# Rename columns to avoid hyphens that may not be compatible with some ggplot functions

dataset_clean <- dataset %>%
  rename(
    sepal_length = `sepal-length`,
    sepal_width = `sepal-width`,
    petal_length = `petal-length`,
    petal_width = `petal-width`
  )

```

```{r}

# Ensure class is a factor
dataset_clean$class <- as.factor(dataset_clean$class)

```


## 5. Create  violin plots

```{r}

# Violin plot for Sepal Length

ggplot(dataset_clean, aes(x = class, y = sepal_length,
                          fill = class)) +
  geom_violin(trim = FALSE, fill = "skyblue") +
  stat_summary(fun = median, geom = "point", color = "red", size = 2) +
  ggtitle("Sepal Length by Class")

```


```{r}

# Violin plot for Sepal Width

ggplot(dataset_clean, aes(x = class, y = sepal_width,
                          fill = class)) +
  geom_violin(trim = FALSE, fill = "lightgreen") +
  stat_summary(fun = median, geom = "point", color = "red", size = 2) +
  ggtitle("Sepal Width by Class")

```

```{r}

# Violin plot for Petal Length

ggplot(dataset_clean, aes(x = class, 
                          y = petal_length,
                          fill = class)) +
  geom_violin(trim = FALSE, fill = "salmon") +
  stat_summary(fun = median, geom = "point", color = "red", size = 2) +
  ggtitle("Petal Length by Class")

```

```{r}

# Violin plot for Petal Width

ggplot(dataset_clean, aes(x = class, 
                          y = petal_width,
                          fill = class)) +
  geom_violin(trim = FALSE, fill = "plum") +
  stat_summary(fun = median, geom = "point", color = "red", size = 2) +
  ggtitle("Petal Width by Class")

```

## 6. Create a correlation heatmap

```{r}

# Create a correlation heatmap

# Compute correlation matrix
cor_matrix <- cor(dataset_clean[, 1:4])

# Melt for ggplot
cor_melt <- melt(cor_matrix)

# Plot correlation heatmap
ggplot(cor_melt, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  geom_text(aes(label = round(value, 2)), color = "white") +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
  labs(title = "Correlation Heatmap of Iris Features", x = "", y = "") +
  theme_minimal()

```

## 7. Prepare data for modeling

```{r}

# Drop 'class' column to get predictors (X)
x <- dataset_clean[, setdiff(names(dataset_clean), "class")]

# Extract 'class' column as target (y)
y <- dataset_clean[["class"]]

# Print shapes (dimensions)
cat(sprintf("X shape: %d x %d | y shape: %d\n", nrow(x), ncol(x), length(y)))
```

## 8. Set training and testing sets

```{r}

# Combine x and y into one dataset if needed
data_all <- cbind(x, class = y)

# Create split object
set.seed(1)
split_obj <- initial_split(data_all, prop = 0.6, strata = class)

# Training and testing sets
train_data <- training(split_obj)
test_data  <- testing(split_obj)

# Optionally extract x/y again
x_train <- train_data[, setdiff(names(train_data), "class")]
y_train <- train_data$class

x_test  <- test_data[, setdiff(names(test_data), "class")]
y_test  <- test_data$class

```

## 9. Compare models

```{r}

# Define training control with 10-fold stratified CV
train_control <- trainControl(
  method = "cv",      # cross-validation
  number = 10,        # 10 folds
  classProbs = FALSE, # no probabilities needed
  summaryFunction = defaultSummary  # default metrics like Accuracy
)

# Model list with caret method names
model_list <- list(
  LR   = "multinom",     # Multinomial Logistic Regression
  LDA  = "lda",          # Linear Discriminant Analysis
  KNN  = "knn",          # K-Nearest Neighbors
  CART = "rpart",        # Decision Tree
  NB   = "naive_bayes",  # Naive Bayes (needs 'naivebayes' package)
  SVC  = "svmLinear"     # Support Vector Classifier
)

```

```{r}

# Store results
results <- list()

# Loop through and train each model
for (model_name in names(model_list)) {
  cat("\nTraining:", model_name, "\n")
  
  fit <- train(
    x = x_train,
    y = y_train,
    method = model_list[[model_name]],
    trControl = train_control,
    metric = "Accuracy"
  )
  
  results[[model_name]] <- fit
  
  # Find the best tuning parameters
  best_params <- fit$bestTune
  
  # Find the row index in results that matches bestTune (works even if bestTune is empty for no tuning)
  if (nrow(best_params) == 0) {
    # No tuning parameters, just take the first row
    idx <- 1
  } else {
    # Match bestTune values in results
    idx <- which(apply(fit$results[, names(best_params), drop=FALSE], 1, function(row) all(row == best_params)))
  }
  
  # Extract accuracy and std dev for best params
  acc <- fit$results$Accuracy[idx]
  std <- fit$results$AccuracySD[idx]
  
  cat(sprintf("%s: Accuracy = %.4f (SD = %.4f)\n", model_name, acc, std))
}

```

## 10. Try for a specific model

```{r}

# Train SVM model (linear kernel)
model <- train(
  x = x_train,
  y = y_train,
  method = "svmLinear",
  trControl = trainControl(method = "none")  # no resampling, just train
)

# Predict on test data
prediction <- predict(model, newdata = x_test)

# View predictions
print(prediction)

```

```{r}

# Calculate accuracy
accuracy <- sum(prediction == y_test) / length(y_test)
cat(sprintf("Test Accuracy: %.4f\n", accuracy))

# Classification report (confusion matrix and stats)
conf_mat <- confusionMatrix(prediction, y_test)
print(conf_mat)

```

## 11. Linear regression workflow

```{r}

# 1. Load the Iris dataset
data(iris)

```

```{r}

# 2. Select features
X <- iris[["Petal.Length"]]  # Independent variable
y <- iris[["Petal.Width"]]   # Dependent variable

```

```{r}

# 3. Split into training and testing sets
set.seed(42)
train_index <- sample(seq_len(nrow(iris)), size = 0.8 * nrow(iris))
X_train <- X[train_index]
X_test  <- X[-train_index]
y_train <- y[train_index]
y_test  <- y[-train_index]

```


```{r}

# 4. Train linear regression model
model <- lm(y_train ~ X_train)

```

```{r}

# 5. Make predictions
y_pred <- predict(model, newdata = data.frame(X_train = X_test))

```

```{r}

# 6. Print model details
cat("Linear Regression Model:\n")
cat(sprintf("Coefficient (m): %.4f\n", coef(model)[2]))
cat(sprintf("Intercept (c): %.4f\n", coef(model)[1]))

```

```{r}

# 7. Calculate R-squared
r_squared <- summary(model)$r.squared
cat(sprintf("R-squared (R²): %.4f\n", r_squared))

```

```{r}

# 8. Visualization

plot_data <- data.frame(
  Petal.Length = X_test,
  Petal.Width = y_test,
  Predicted = y_pred
)

ggplot(plot_data, aes(x = Petal.Length, y = Petal.Width)) +
  geom_point(color = "blue", size = 2, alpha = 0.6) +
  geom_line(aes(y = Predicted), color = "red", size = 1.2) +
  labs(
    title = "Linear Regression on Iris Dataset",
    x = "Petal Length (cm)",
    y = "Petal Width (cm)"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_y_continuous(sec.axis = dup_axis(name = NULL)) +
  guides(color = "none")

```

## 12. Support Vector Classifier (SVC) with a linear kernel

```{r}

# 1. Load the Iris dataset
data(iris)

```

```{r}

# 2. Keep only the first two features for visualization
iris_subset <- iris[, c("Sepal.Length", "Sepal.Width", "Species")]

```

```{r}

# 3. Split into train and test sets (80/20)
set.seed(42)
train_index <- createDataPartition(iris_subset$Species, p = 0.8, list = FALSE)
train_data <- iris_subset[train_index, ]
test_data  <- iris_subset[-train_index, ]

```

```{r}

# 4. Train the SVM model with a linear kernel
model <- svm(Species ~ Sepal.Length + Sepal.Width, 
             data = train_data, 
             kernel = "linear", 
             cost = 1.0, 
             scale = TRUE)
```

```{r}

# 5. Make predictions and evaluate
pred <- predict(model, newdata = test_data)

cat("Support Vector Classifier Model Performance:\n")
accuracy <- mean(pred == test_data$Species)
cat(sprintf("Accuracy: %.2f\n\n", accuracy))

```

```{r}

# Classification report
conf_mat <- confusionMatrix(pred, test_data$Species)
print(conf_mat)

```

```{r}

# 6. Visualize the decision boundaries
# Create a grid to evaluate model
x_min <- min(iris_subset$Sepal.Length) - 0.5
x_max <- max(iris_subset$Sepal.Length) + 0.5
y_min <- min(iris_subset$Sepal.Width) - 0.5
y_max <- max(iris_subset$Sepal.Width) + 0.5

```

```{r}

# Resolution of the grid
h <- 0.02
grid <- expand.grid(
  Sepal.Length = seq(x_min, x_max, by = h),
  Sepal.Width  = seq(y_min, y_max, by = h)
)

```

```{r}

# Predict the class for each point in the grid
grid$Species <- predict(model, newdata = grid)

```

```{r}

# 7. Plot
ggplot() +
  geom_tile(data = grid, aes(x = Sepal.Length, y = Sepal.Width, fill = Species), alpha = 0.3) +
  geom_point(data = iris_subset, 
             aes(x = Sepal.Length, y = Sepal.Width, color = Species),
             size = 2, shape = 21, stroke = 1) +
  scale_fill_manual(values = c("#D55E00", "#0072B2", "#009E73")) +
  scale_color_manual(values = c("#D55E00", "#0072B2", "#009E73")) +
  labs(title = "SVC Decision Boundary on Iris Dataset",
       x = "Sepal Length (cm)",
       y = "Sepal Width (cm)") +
  theme_minimal() +
  theme(legend.position = "right")

```

## 13. Linear Discriminant Analysis (LDA)

```{r}

# 1. Load the Iris dataset
data(iris)

```

```{r}

# 2. Set features (X) and target (y)
X <- iris[, 1:4]            # All 4 numeric features
y <- iris$Species           # Target variable

```

```{r}

# 3. Split data into training and testing sets (70/30)
set.seed(42)
train_index <- createDataPartition(y, p = 0.7, list = FALSE)
X_train <- X[train_index, ]
y_train <- y[train_index]
X_test  <- X[-train_index, ]
y_test  <- y[-train_index]

```

```{r}

# 4. Train LDA model (and transform training data)
lda_model <- lda(Species ~ ., data = iris[train_index, ])
X_train_lda <- predict(lda_model, X_train)$x

```

```{r}

# 5. Predict on test set
lda_pred <- predict(lda_model, X_test)
y_pred <- lda_pred$class

```

```{r}

# 6. Evaluate performance
cat("Linear Discriminant Analysis (LDA) Model Performance:\n")
accuracy <- mean(y_pred == y_test)
cat(sprintf("Accuracy: %.2f\n\n", accuracy))

conf_matrix <- confusionMatrix(y_pred, y_test)
print(conf_matrix)

```

```{r}

# 7. Visualize the LDA projection (entire dataset)
lda_all <- lda(Species ~ ., data = iris)
lda_values <- predict(lda_all)$x  # Projected values

# Combine with original labels for plotting
plot_data <- data.frame(lda_values, Species = iris$Species)

```

```{r}

# 8. Plot
ggplot(plot_data, aes(x = LD1, y = LD2, color = Species)) +
  geom_point(size = 3, alpha = 0.8) +
  labs(
    title = "LDA of Iris Dataset",
    x = "Linear Discriminant 1",
    y = "Linear Discriminant 2"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_viridis_d(option = "viridis")

```

